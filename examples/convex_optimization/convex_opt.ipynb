{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from llmize import OPRO, HLMEA, HLMSA\n",
    "import llmize\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red,\n",
      "Violets are blue,\n",
      "Sugar is sweet,\n",
      "And I’m feeling cute! \n",
      "\n",
      "Would you like me to continue the rhyme? Or perhaps you have a different request?\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemma-3-1b-it\",\n",
    "    contents=\"Roses are red...\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.5\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(llmize.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_convex_penalty(x):\n",
    "    \"\"\"\n",
    "    Objective function for the Convex Optimization problem with penalties.\n",
    "    The function is minimized.\n",
    "    \"\"\"\n",
    "    x1, x2 = x\n",
    "    f = (x1 - 3)**2 + (x2 + 2)**2 + np.sin(x1 + x2) + 4\n",
    "    \n",
    "    # Constraint violations\n",
    "    penalty = 0\n",
    "    large_penalty = 1e6  # Large penalty value\n",
    "\n",
    "    if x1 < 0 or x1 > 5:\n",
    "        penalty += large_penalty\n",
    "    if x2 < 0 or x2 > 5:\n",
    "        penalty += large_penalty\n",
    "\n",
    "    return f + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[np.float64(0.0), np.float64(0.0)], [np.float64(0.0), np.float64(1.5)], [np.float64(0.0), np.float64(3.0)], [np.float64(0.0), np.float64(4.5)], [np.float64(1.5), np.float64(0.0)], [np.float64(1.5), np.float64(1.5)], [np.float64(1.5), np.float64(3.0)], [np.float64(1.5), np.float64(4.5)], [np.float64(3.0), np.float64(0.0)], [np.float64(3.0), np.float64(1.5)], [np.float64(3.0), np.float64(3.0)], [np.float64(3.0), np.float64(4.5)], [np.float64(4.5), np.float64(0.0)], [np.float64(4.5), np.float64(1.5)], [np.float64(4.5), np.float64(3.0)], [np.float64(4.5), np.float64(4.5)]]\u001b[0m\n",
      "\u001b[0m[np.float64(17.0), np.float64(26.247494986604053), np.float64(38.141120008059865), np.float64(54.2724698823349), np.float64(11.247494986604053), np.float64(18.641120008059865), np.float64(30.272469882334903), np.float64(48.22058450180107), np.float64(8.141120008059868), np.float64(15.272469882334903), np.float64(28.720584501801074), np.float64(47.18799997677474), np.float64(9.272469882334903), np.float64(18.220584501801074), np.float64(32.18799997677474), np.float64(48.912118485241756)]\u001b[0m\n",
      "\u001b[0m16\u001b[0m \u001b[0m16\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Generate random solutions (list of lists) and scores (list of floats)\n",
    "num_samples = 4\n",
    "batch_size = num_samples**2\n",
    "x1_range = np.linspace(0, 4.5, num_samples)\n",
    "x2_range = np.linspace(0, 4.5, num_samples)\n",
    "solutions = []\n",
    "scores = []\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_samples):\n",
    "        solutions.append([x1_range[i], x2_range[j]])\n",
    "        scores.append(objective_convex_penalty([x1_range[i], x2_range[j]]))\n",
    "\n",
    "print(solutions)\n",
    "print(scores)\n",
    "print(len(solutions), len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\u001b[0m\n",
      "\u001b[0mPrompt:\u001b[0m\n",
      "\u001b[0mProblem: Convex Optimization\n",
      "-----------------------------------------------------\n",
      "Objective: Minimize the function\n",
      "    f(x1, x2) = (x1 - 3)^2 + (x2 + 2)^2 + sin(x1 + x2) + 4\n",
      "\n",
      "Subject to constraints:\n",
      "    0 ≤ x1 ≤ 5\n",
      "    0 ≤ x2 ≤ 5\n",
      "Below are some examples of solutions and their scores:\n",
      "\n",
      "<sol> 0.0,0.0 <\\sol>\n",
      "score: 17.000\n",
      "\n",
      "<sol> 0.0,1.5 <\\sol>\n",
      "score: 26.247\n",
      "\n",
      "<sol> 0.0,3.0 <\\sol>\n",
      "score: 38.141\n",
      "\n",
      "<sol> 0.0,4.5 <\\sol>\n",
      "score: 54.272\n",
      "\n",
      "<sol> 1.5,0.0 <\\sol>\n",
      "score: 11.247\n",
      "\n",
      "<sol> 1.5,1.5 <\\sol>\n",
      "score: 18.641\n",
      "\n",
      "<sol> 1.5,3.0 <\\sol>\n",
      "score: 30.272\n",
      "\n",
      "<sol> 1.5,4.5 <\\sol>\n",
      "score: 48.221\n",
      "\n",
      "<sol> 3.0,0.0 <\\sol>\n",
      "score: 8.141\n",
      "\n",
      "<sol> 3.0,1.5 <\\sol>\n",
      "score: 15.272\n",
      "\n",
      "<sol> 3.0,3.0 <\\sol>\n",
      "score: 28.721\n",
      "\n",
      "<sol> 3.0,4.5 <\\sol>\n",
      "score: 47.188\n",
      "\n",
      "<sol> 4.5,0.0 <\\sol>\n",
      "score: 9.272\n",
      "\n",
      "<sol> 4.5,1.5 <\\sol>\n",
      "score: 18.221\n",
      "\n",
      "<sol> 4.5,3.0 <\\sol>\n",
      "score: 32.188\n",
      "\n",
      "<sol> 4.5,4.5 <\\sol>\n",
      "score: 48.912\n",
      "\n",
      "\n",
      "Generate exactly 5 new solutions that:\n",
      "- Are distinct from all previous solutions.\n",
      "- Have lower scores than the lowest provided.\n",
      "- Respect the relationships based on logical reasoning.\n",
      "\n",
      "Each solution should start with <sol> and end with </sol> with a comma between parameters.\n",
      "Make sure the length of solutions match examples given. Don't guess for the scores as they will be calculated by an objective function.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0m======================================================================================================================================================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "with open(\"convex_problem.txt\", \"r\") as f:\n",
    "    problem_text = f.read()\n",
    "\n",
    "# Initialize the OPRO optimizer\n",
    "opro = OPRO(problem_text=problem_text, obj_func=objective_convex_penalty,\n",
    "            llm_model=\"gemma-3-27b-it\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "prompt = opro.get_sample_prompt(init_samples=solutions, init_scores=scores, optimization_type=\"minimize\")\n",
    "response = opro.get_sample_response(prompt)\n",
    "\n",
    "llmize.utils.pretty_print(prompt=prompt, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmize.callbacks import EarlyStopping, AdaptTempOnPlateau, OptimalScoreStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "earlystop_callback = EarlyStopping(monitor='best_score', min_delta=0.001, patience=50, verbose=1)\n",
    "\n",
    "# Define the optimal score stopping callback\n",
    "optimal_score_callback = OptimalScoreStopping(optimal_score=7.90, tolerance=0.01)\n",
    "\n",
    "# Define the temperature adaptation callback\n",
    "adapt_temp_callback = AdaptTempOnPlateau(monitor='best_score', init_temperature=1.0, min_delta=0.001, patience=20, factor=1.1, max_temperature=1.9, verbose=1)\n",
    "\n",
    "callbacks = [earlystop_callback, optimal_score_callback, adapt_temp_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mRunning OPRO optimization with 250 steps and batch size 16...\u001b[0m\n",
      "\u001b[0m\u001b[37mStep 0 - Best Initial Score: 8.141, Average Initial Score: 28.247\u001b[0m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Number of solutions parsed is less than batch size. Retrying...\u001b[0m\n",
      "\u001b[0m\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Number of solutions parsed is less than batch size. Retrying...\u001b[0m\n",
      "\u001b[0m\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Number of solutions parsed is less than batch size. Retrying...\u001b[0m\n",
      "\u001b[0m\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Number of solutions parsed is less than batch size. Retrying...\u001b[0m\n",
      "\u001b[0m\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Number of solutions parsed is less than batch size. Retrying...\u001b[0m\n",
      "\u001b[0m\u001b[31mERROR - An error occurred: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemma-3-27b is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}\u001b[0m\n",
      "\u001b[0m\u001b[33mWARNING - Max retries reached. Could not complete the request.\u001b[0m\n",
      "\u001b[0m\u001b[35m\u001b[1mCRITICAL - Failed to generate solutions after multiple attempts.\u001b[0m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to generate solutions after multiple attempts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mopro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolutions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anvil/projects/pur230006/data/rizki/llmize/llmize/utils/decorators.py:42\u001b[0m, in \u001b[0;36mcheck_init.<locals>.inner\u001b[0;34m(self, init_samples, init_scores, num_steps, batch_size, temperature, callbacks, verbose, parallel_n_jobs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     log_critical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose must be an integer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose must be an integer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_n_jobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anvil/projects/pur230006/data/rizki/llmize/llmize/utils/decorators.py:62\u001b[0m, in \u001b[0;36mtime_it.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record start time\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call the original function\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     log_info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anvil/projects/pur230006/data/rizki/llmize/llmize/base.py:91\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[0;34m(self, init_samples, init_scores, num_steps, batch_size, temperature, callbacks, verbose, parallel_n_jobs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@check_init\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@time_it\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, init_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, init_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     75\u001b[0m              temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, parallel_n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Run the optimization algorithm to minimize the objective function.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    - results (dict): A dictionary containing the best solution, best score, best score history, best score per step, and average score per step.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimization_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mminimize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_n_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_n_jobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anvil/projects/pur230006/data/rizki/llmize/llmize/methods/opro.py:125\u001b[0m, in \u001b[0;36mOPRO.optimize\u001b[0;34m(self, init_samples, init_scores, num_steps, batch_size, temperature, callbacks, verbose, optimization_type, parallel_n_jobs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     log_debug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample pairs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_pairs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_prompt(batch_size, example_pairs, optimization_type)\n\u001b[0;32m--> 125\u001b[0m solution_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_solutions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m best_score, best_solution, step_scores, best_step_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_solutions(solution_array, best_solution,\n\u001b[1;32m    129\u001b[0m                                                                   optimization_type, verbose, best_score, parallel_n_jobs)\n\u001b[1;32m    130\u001b[0m new_pairs \u001b[38;5;241m=\u001b[39m parse_pairs(solution_array, step_scores)\n",
      "File \u001b[0;32m/anvil/projects/pur230006/data/rizki/llmize/llmize/base.py:211\u001b[0m, in \u001b[0;36mOptimizer._generate_solutions\u001b[0;34m(self, client, prompt, temperature, batch_size, verbose, max_retries, hp_parse)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_retries:\n\u001b[1;32m    210\u001b[0m         log_critical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate solutions after multiple attempts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to generate solutions after multiple attempts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(solution_array) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[1;32m    214\u001b[0m     log_warning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of solutions parsed is greater than batch size. Removing extra solutions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to generate solutions after multiple attempts."
     ]
    }
   ],
   "source": [
    "results = opro.minimize(init_samples=solutions, init_scores=scores, num_steps=250, batch_size=16, callbacks=callbacks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
