Task: Optimize the hyperparameters for a feedforward neural network model to classify handwritten digits in the MNIST dataset.

Model Architecture:

Input Layer: 28x28 pixel images (flattened to 784 inputs)
Hidden Layer: A single dense layer with configurable number of units, activated by ReLU
Dropout Layer: Added for regularization to prevent overfitting
Output Layer: Dense layer with 10 units (representing digits 0-9), softmax activation for multi-class classification

Hyperparameters to Optimize:

units: The number of neurons in the hidden layer
dropout: The dropout rate (probability of randomly deactivating neurons during training)
learning_rate: The learning rate for the Adam optimizer

Training Setup:

Optimizer: Adam
Loss Function: Sparse categorical cross-entropy (ideal for multi-class classification tasks)
Metrics: Accuracy
Epochs: 5 epochs
Validation data used during training

Data Processing:

Dataset: MNIST
Pixel values normalized to range [0, 1] by dividing by 255.0
Request: Suggest optimized values for the following hyperparameters based on your understanding of the dataset and model:

Number of neurons in the hidden layer (units)
Dropout rate (dropout)
Learning rate (learning_rate)




